{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasanthrohith/Resume_extraction_LangChain/blob/main/App_ResumeExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugqfZSUw0bXj"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install unstructured\n",
        "!pip install tiktoken\n",
        "!pip install pypdf2\n",
        "!pip install streamlit -q\n",
        "!pip install aspose-words\n",
        "# !pip install faiss-cpu\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0YUpMXX1G-g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki1iDOAk2nUM"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Main cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQLWs1Fm1qnU",
        "outputId": "34953ba8-2a2c-4fe0-842c-7063d8bf3cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import PyPDF2\n",
        "import os\n",
        "import time\n",
        "import aspose.words as aw\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain import VectorDBQA\n",
        "\n",
        "\n",
        "\n",
        "openai_key=\"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\"\n",
        "\n",
        "#--------------------Prompt template---------------------------\n",
        "\n",
        "\n",
        "template=\"\"\"\n",
        "You will be provided with text which are extracted from resume\n",
        "your goal is to extract the below detail from the provided text in the below format:\n",
        "\"Name\" : full name,\n",
        "\"mobile_number\" : number,\n",
        "\"Work_experience\" : experience,\n",
        "\"projects\": projects,\n",
        "\"email_s\": email\n",
        "\n",
        "if there is any field mentioned above not in text you should simply mention NA\n",
        "\n",
        "You should not consider skills, area of interest and extra curricular activities as work experience\n",
        "give a brief details about the work experience\n",
        "if there is no work experience simply mention NA\n",
        "\n",
        "in addition you should predict the gender of that person and mention it as\n",
        "\"gender\": gender\n",
        "\n",
        "refer the below exaple for your work:\n",
        "\"Name\" : surya krishnan,\n",
        "\"mobile_number\" : 9988445568,\n",
        "\"Work_experience\" : 2 years worked at google,\n",
        "\"projects\":'petrofac - product deployment and server management (development,uat and production); prdp - product development and support,\n",
        "\"email\": surya@gmail.com\n",
        "\"gender\": male\n",
        "if there is any field mentioned above not in text you should simply mention NA\n",
        "\n",
        "\n",
        "Below is the text:\n",
        "text: {text}\n",
        "\n",
        "Your resopnse:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt=PromptTemplate(input_variables=[\"text\"],\n",
        "                      template=template)\n",
        "\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"Resume_Extraction\",page_icon=\"🔍\")\n",
        "st.header(\"Get your resume highlights\")\n",
        "# st.write(\"upload your file\")\n",
        "\n",
        "\n",
        "#--------------------Functions---------------------------\n",
        "\n",
        "\n",
        "def model(openai_key):\n",
        "    llm=OpenAI(temperature=0,openai_api_key=openai_key)\n",
        "    return llm\n",
        "\n",
        "def path():\n",
        "    file_path_=\"\"\n",
        "    #st.write(file_path_)\n",
        "    time.sleep(3)\n",
        "    try:\n",
        "        while True:\n",
        "            current_path=os.getcwd()\n",
        "            file_name=\"saved_file.txt\"\n",
        "            file_path=os.path.join(current_path,file_name)\n",
        "            file_path_ = file_path\n",
        "            break\n",
        "    except Exception as e:\n",
        "        path()\n",
        "    return file_path_\n",
        "\n",
        "\n",
        "\n",
        "def file_summary(file_doc):\n",
        "    splitter=RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=0)\n",
        "    docs=splitter.split_documents(file_doc)\n",
        "\n",
        "    summarizer=load_summarize_chain(llm,chain_type=\"map_reduce\",verbose=False)\n",
        "    summary=summarizer.run(docs)\n",
        "\n",
        "    return summary\n",
        "\n",
        "if \"result\" not in st.session_state:\n",
        "    st.session_state.result=None\n",
        "\n",
        "\n",
        "uploaded_file = st.file_uploader(\"upload your file\", type=['pdf','docx','doc'])\n",
        "\n",
        "menubar=[\"Home\",\"Project Q&A\"]\n",
        "choices=st.sidebar.selectbox(\"Menu\",menubar)\n",
        "\n",
        "if choices==\"Home\":\n",
        "\n",
        "    if uploaded_file:\n",
        "\n",
        "        if uploaded_file.type == \"pdf\":\n",
        "            #st.write(uploaded_file.type)\n",
        "\n",
        "\n",
        "            #--------------------File handling---------------------------\n",
        "            text=\"\"\n",
        "            pdf1 = PyPDF2.PdfReader(uploaded_file)\n",
        "            pages = len(pdf1.pages)\n",
        "            print(\"Number of pages - \",pages)\n",
        "            for i in range(pages):\n",
        "                page=pdf1.pages[i]\n",
        "                text+=page.extract_text()\n",
        "\n",
        "            file_path=path()\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                os.remove(file_path)\n",
        "                with open(\"saved_file.txt\", \"a\") as f:\n",
        "                    f.write(text)\n",
        "                    #st.success(\"File saved successfully.\")\n",
        "\n",
        "            else:\n",
        "                with open(\"saved_file.txt\", \"a\") as f:\n",
        "                    f.write(text)\n",
        "                    #st.success(\"File saved successfully.\")\n",
        "\n",
        "            loader=TextLoader(file_path)\n",
        "            file_doc=loader.load()\n",
        "\n",
        "        else:\n",
        "            uploaded_docx = aw.Document(uploaded_file)\n",
        "            save_docx = uploaded_docx.save(\"output.txt\")\n",
        "            docx_path = os.path.abspath(\"output.txt\")\n",
        "            loader = TextLoader(docx_path)\n",
        "            file_doc = loader.load()\n",
        "\n",
        "\n",
        "        #--------------------Fetching Details---------------------------\n",
        "\n",
        "        llm=model(openai_key)\n",
        "\n",
        "        try:\n",
        "            prompted=prompt.format(text=file_doc)\n",
        "            print(prompted)\n",
        "            st.session_state.result=llm(prompted)\n",
        "            print(st.session_state.result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"--------------------sumarizing\")\n",
        "            text_splitter=RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=0)\n",
        "            file_chunks=text_splitter.split_documents(file_doc)\n",
        "\n",
        "            summarizer=load_summarize_chain(llm,chain_type=\"map_reduce\",verbose=True)\n",
        "            file_doc_summary=summarizer.run(file_chunks)\n",
        "\n",
        "            prompted=prompt.format(text=file_doc_summary)\n",
        "            st.session_state.result=llm(prompted)\n",
        "\n",
        "\n",
        "        #st.write(st.session_state.result)\n",
        "\n",
        "        #--------------------Cleaning result---------------------------\n",
        "\n",
        "\n",
        "\n",
        "        splitted=st.session_state.result.split('\\n')\n",
        "        # print(splitted)\n",
        "\n",
        "        #split(\\n)=> to separate every entities in newline so that easily iterated through\n",
        "\n",
        "        dict_entities={}\n",
        "        # using dict to collect all the entities for easy accessibility\n",
        "\n",
        "        for i in splitted:\n",
        "            i=i.lower()\n",
        "            # lower() => to make normalize the text\n",
        "\n",
        "            entities=i.split(':')\n",
        "            # split(':') => to separate label(entities[0]) and values(entities[1])\n",
        "\n",
        "            # print(entities)\n",
        "\n",
        "            #below ladder if will help us to extract every entities and add to dict_entities based on it's label\n",
        "\n",
        "            if \"name\" in entities[0]:\n",
        "                print(f\"name: {entities[1]}\")\n",
        "                dict_entities['name']=entities[1].strip()\n",
        "\n",
        "            elif \"mobile_number\" in entities[0]:\n",
        "                print(f\"mobile_number: {entities[1]}\")\n",
        "                dict_entities['mobile_number']=entities[1].strip()\n",
        "\n",
        "            elif \"work_experience\" in entities[0]:\n",
        "                print(f\"Work_experience: {entities[1:]}\")\n",
        "                dict_entities['Work_experience']=entities[1].strip()\n",
        "\n",
        "            elif \"projects\" in entities[0]:\n",
        "                print(f\"projects: {entities[1:]}\")\n",
        "                dict_entities['projects']=entities[1].strip()\n",
        "\n",
        "            elif \"email\" in entities[0]:\n",
        "                print(f\"email: {entities[1]}\")\n",
        "                dict_entities['email']=entities[1].strip()\n",
        "\n",
        "            elif \"gender\" in entities[0]:\n",
        "                print(f\"gender: {entities[1]}\")\n",
        "                dict_entities['gender']=entities[1].strip()\n",
        "\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "\n",
        "        #st.write(dict_entities)\n",
        "        #st.markdown(dict_entities)\n",
        "\n",
        "\n",
        "\n",
        "        #--------------------output to front-end---------------------------\n",
        "\n",
        "        st.markdown(\"### Details\")\n",
        "        col1,col2=st.columns(2)\n",
        "        with col1:\n",
        "            # st.markdown(\"**Number of pages**\")\n",
        "            st.text_input(label=\"Name\",value=dict_entities['name'])\n",
        "        with col2:\n",
        "            st.text_input(label=\"Phone Number\",value=dict_entities['mobile_number'].replace(\" \",\"\"))\n",
        "\n",
        "\n",
        "        st.text_area(label=\"Work experience\",value=dict_entities['Work_experience'])\n",
        "\n",
        "        st.text_area(label=\"Projects\",value=dict_entities['projects'])\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            # st.markdown(\"**Number of pages**\")\n",
        "            st.text_input(label=\"Email\", value=dict_entities['email'])\n",
        "        with col2:\n",
        "            st.text_input(label=\"Gender\", value=dict_entities['gender'])\n",
        "\n",
        "\n",
        "        #--------------------summary of uploaded file---------------------------\n",
        "        st.write(\"\")\n",
        "        st.markdown(\"### Summary\")\n",
        "\n",
        "        summary=file_summary(file_doc)\n",
        "        st.write(summary)\n",
        "\n",
        "        #--------------------Interact with uploaded file---------------------------\n",
        "\n",
        "        st.write(\"\")\n",
        "        st.markdown(\"Ask a question\")\n",
        "        #user_question=\"what is the name of the person\"\n",
        "        user_question=st.text_input(label=\" \")\n",
        "        if len(user_question)>2:\n",
        "            user_question=str(user_question)\n",
        "\n",
        "            embeddings=OpenAIEmbeddings(openai_api_key=openai_key)\n",
        "            docsearch=Chroma.from_documents(file_doc,embeddings)\n",
        "\n",
        "            qa=VectorDBQA.from_chain_type(llm=llm,chain_type=\"stuff\",vectorstore=docsearch,return_source_documents=False)\n",
        "            answer=qa({\"query\":user_question})\n",
        "\n",
        "            st.write(answer['result'])\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "elif choices==\"Project Q&A\":\n",
        "\n",
        "\n",
        "    st.subheader(\"Project Q&A\")\n",
        "\n",
        "    if uploaded_file:\n",
        "        if uploaded_file.type == \"pdf\":\n",
        "            #st.write(uploaded_file.type)\n",
        "\n",
        "\n",
        "            #--------------------File handling---------------------------\n",
        "            text=\"\"\n",
        "            pdf1 = PyPDF2.PdfReader(uploaded_file)\n",
        "            pages = len(pdf1.pages)\n",
        "            print(\"Number of pages - \",pages)\n",
        "            for i in range(pages):\n",
        "                page=pdf1.pages[i]\n",
        "                text+=page.extract_text()\n",
        "\n",
        "\n",
        "            #current_path=os.getcwd()\n",
        "            #file_name=\"saved_file.txt\"\n",
        "            #file_path=os.path.join(current_path,file_name)\n",
        "\n",
        "            file_path=path()\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                os.remove(file_path)\n",
        "                with open(\"saved_file.txt\", \"a\") as f:\n",
        "                    f.write(text)\n",
        "                    #st.success(\"File saved successfully.\")\n",
        "\n",
        "            else:\n",
        "                with open(\"saved_file.txt\", \"a\") as f:\n",
        "                    f.write(text)\n",
        "                    #st.success(\"File saved successfully.\")\n",
        "\n",
        "            #file_path=path()\n",
        "            #st.write(file_path)\n",
        "\n",
        "            loader=TextLoader(file_path)\n",
        "            file_doc=loader.load()\n",
        "\n",
        "\n",
        "        else:\n",
        "            d=aw.Document(uploaded_file)\n",
        "            b=d.save(\"output.txt\")\n",
        "            b=os.path.abspath(\"output.txt\")\n",
        "            loader = TextLoader(b)\n",
        "            file_doc=loader.load()\n",
        "\n",
        "        llm=model(openai_key)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Code - 1/7/23"
      ],
      "metadata": {
        "id": "Si0xIP8-8WZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import PyPDF2\n",
        "import os\n",
        "import time\n",
        "import aspose.words as aw\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain import VectorDBQA\n",
        "from openai.error import RateLimitError\n",
        "\n",
        "openai_key=\"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\"\n",
        "\n",
        "\n",
        "#--------------------Prompt template---------------------------\n",
        "\n",
        "\n",
        "template=\"\"\"\n",
        "You will be provided with text which are extracted from resume\n",
        "your goal is to extract the below detail from the provided text in the below format:\n",
        "\"Name\" : full name,\n",
        "\"mobile_number\" : number,\n",
        "\"Work_experience\" : experience,\n",
        "\"projects\": projects,\n",
        "\"email_s\": email,\n",
        "\"gender\": gender\n",
        "\n",
        "if there is any field mentioned above not in text you should simply mention NA\n",
        "\n",
        "You should not consider skills, area of interest and extra curricular activities as work experience\n",
        "give a brief details about the work experience\n",
        "if there is no work experience simply mention NA\n",
        "\n",
        "in addition you should predict the gender of that person and mention it as\n",
        "\"gender\": gender\n",
        "\n",
        "refer the below exaple for your work:\n",
        "\"Name\" : surya krishnan,\n",
        "\"mobile_number\" : 9988445568,\n",
        "\"Work_experience\" : 2 years worked at google,\n",
        "\"projects\":'petrofac - product deployment and server management (development,uat and production); prdp - product development and support,\n",
        "\"email\": surya@gmail.com\n",
        "\"gender\": male\n",
        "if there is any field mentioned above not in text you should simply mention NA\n",
        "\n",
        "\n",
        "Below is the text:\n",
        "text: {text}\n",
        "\n",
        "Your resopnse:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "template_project_qna = \"\"\"\n",
        "\n",
        "Your role is an interviewer\n",
        "\n",
        "You will be provided with a candidate's job profile details as text\n",
        "\n",
        "your task is to:\n",
        "- ask questions only from the projects mentioned on his profile\n",
        "- you have to ask only 8 questions in total, don't ask not more than 8\n",
        "- you should ask those questions from any one project mentiond on his profile\n",
        "\n",
        "Below is the text:\n",
        "text: {text}\n",
        "\n",
        "Your resopnse:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt=PromptTemplate(input_variables=[\"text\"],\n",
        "                      template=template)\n",
        "\n",
        "prompt_project_qna=PromptTemplate(input_variables=[\"text\"],\n",
        "                                  template=template_project_qna)\n",
        "\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"Resume_Extraction\",page_icon=\"🔍\")\n",
        "# st.header(\"Get your resume highlights\")\n",
        "# st.write(\"upload your file\")\n",
        "\n",
        "\n",
        "#--------------------Functions---------------------------\n",
        "\n",
        "\n",
        "def model(openai_key):\n",
        "    try:\n",
        "        llm=OpenAI(temperature=0,openai_api_key=openai_key)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        st.write(\"please check your Openai API key and try again\")\n",
        "\n",
        "\n",
        "    return llm\n",
        "\n",
        "def path():\n",
        "    file_path_=\"\"\n",
        "    #st.write(file_path_)\n",
        "    time.sleep(3)\n",
        "    try:\n",
        "        while True:\n",
        "            current_path=os.getcwd()\n",
        "            file_name=\"saved_file.txt\"\n",
        "            file_path=os.path.join(current_path,file_name)\n",
        "            file_path_ = file_path\n",
        "            break\n",
        "    except Exception as e:\n",
        "        path()\n",
        "    return file_path_\n",
        "\n",
        "\n",
        "\n",
        "def file_summary(file_doc):\n",
        "    splitter=RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=0)\n",
        "    docs=splitter.split_documents(file_doc)\n",
        "    try:\n",
        "        summarizer=load_summarize_chain(llm,chain_type=\"map_reduce\",verbose=False)\n",
        "        summary=summarizer.run(docs)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        st.write(\"please check your Openai API key and try again\")\n",
        "        st.stop()\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "uploaded_file = st.file_uploader(\"\", type=['pdf','docx','doc'],accept_multiple_files=False)\n",
        "\n",
        "menubar=[\"Home\",\"Chat with Resume/CV\",\"Project Q&A\"]\n",
        "choices=st.sidebar.selectbox(\"Menu\",menubar)\n",
        "\n",
        "if \"file_doc\" not in st.session_state:      #\n",
        "    st.session_state.file_doc=None              #=> Help us to store the variable from rerun vanishing\n",
        "\n",
        "if \"result\" not in st.session_state:\n",
        "    st.session_state.result=None\n",
        "\n",
        "if \"summary\" not in st.session_state:       #\n",
        "    st.session_state.summary=None\n",
        "\n",
        "if \"questions\" not in st.session_state:       #\n",
        "    st.session_state.questions=None\n",
        "\n",
        "if \"user_answer\" not in st.session_state:\n",
        "    st.session_state.user_answer=None\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------------------Home-----------------------------\n",
        "\n",
        "if choices==\"Home\":\n",
        "\n",
        "    st.subheader(\"Get your resume highlights 🔍\")\n",
        "    # uploaded_file = st.file_uploader(\"\", type=['pdf', 'docx', 'doc'])\n",
        "\n",
        "    if uploaded_file:\n",
        "\n",
        "        if uploaded_file.type == \"pdf\":\n",
        "            #st.write(uploaded_file.type)\n",
        "\n",
        "\n",
        "            #--------------------File handling---------------------------\n",
        "            text=\"\"\n",
        "            pdf1 = PyPDF2.PdfReader(uploaded_file)\n",
        "            pages = len(pdf1.pages)\n",
        "            print(\"Number of pages - \",pages)\n",
        "            for i in range(pages):\n",
        "                page=pdf1.pages[i]\n",
        "                text+=page.extract_text()\n",
        "\n",
        "            file_path=path()\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                os.remove(file_path)\n",
        "                with open(\"saved_file.txt\", \"a\") as f:\n",
        "                    f.write(text)\n",
        "                    #st.success(\"File saved successfully.\")\n",
        "\n",
        "            else:\n",
        "                with open(\"saved_file.txt\", \"a\") as f:\n",
        "                    f.write(text)\n",
        "                    #st.success(\"File saved successfully.\")\n",
        "\n",
        "            loader=TextLoader(file_path)\n",
        "            st.session_state.file_doc=loader.load()\n",
        "\n",
        "        else:\n",
        "            try:\n",
        "                uploaded_docx = aw.Document(uploaded_file)\n",
        "                save_docx = uploaded_docx.save(\"output.txt\")\n",
        "                docx_path = os.path.abspath(\"output.txt\")\n",
        "                loader = TextLoader(docx_path)\n",
        "                st.session_state.file_doc = loader.load()\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                st.markdown(\"Please check the uploaded file\")\n",
        "                st.stop()\n",
        "\n",
        "\n",
        "        #--------------------Fetching Details---------------------------\n",
        "\n",
        "        llm=model(openai_key)\n",
        "\n",
        "        try:\n",
        "            prompted=prompt.format(text=st.session_state.file_doc)\n",
        "            print(prompted)\n",
        "            try:\n",
        "                st.session_state.result=llm(prompted)\n",
        "                print(st.session_state.result)\n",
        "            except RateLimitError as e:\n",
        "                print(\"ERROR----------------- In Fetching Details\")\n",
        "                print(e)\n",
        "                st.markdown(\"### Please check your openai API key\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"\\n--------------------sumarizing\\n\")\n",
        "            # text_splitter=RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=0)\n",
        "            # file_chunks=text_splitter.split_documents(st.session_state.file_doc)\n",
        "            #\n",
        "            # summarizer=load_summarize_chain(llm,chain_type=\"map_reduce\",verbose=True)\n",
        "            # file_doc_summary=summarizer.run(file_chunks)\n",
        "            file_doc_summary = file_summary(st.session_state.file_doc)\n",
        "\n",
        "            prompted=prompt.format(text=file_doc_summary)\n",
        "            try:\n",
        "                result=llm(prompted)\n",
        "            except RateLimitError as e:\n",
        "                print(\"ERROR----------------- In Fetching Details summarizing\")\n",
        "                print(e)\n",
        "                st.markdown(\"### Please check your openai API key\")\n",
        "\n",
        "\n",
        "        #st.write(result)\n",
        "\n",
        "        #--------------------Cleaning result---------------------------\n",
        "\n",
        "        try:\n",
        "            splitted=st.session_state.result.split('\\n')\n",
        "            # print(splitted)\n",
        "\n",
        "            #split(\\n)=> to separate every entities in newline so that easily iterated through\n",
        "\n",
        "            dict_entities={}\n",
        "            # using dict to collect all the entities for easy accessibility\n",
        "\n",
        "            for i in splitted:\n",
        "                i=i.lower()\n",
        "                # lower() => to make normalize the text\n",
        "\n",
        "                entities=i.split(':')\n",
        "                # split(':') => to separate label(entities[0]) and values(entities[1])\n",
        "\n",
        "                # print(entities)\n",
        "\n",
        "                #below ladder if will help us to extract every entities and add to dict_entities based on it's label\n",
        "\n",
        "                if \"name\" in entities[0]:\n",
        "                    print(f\"name: {entities[1]}\")\n",
        "                    dict_entities['name']=entities[1].strip()\n",
        "\n",
        "                elif \"mobile_number\" in entities[0]:\n",
        "                    print(f\"mobile_number: {entities[1]}\")\n",
        "                    dict_entities['mobile_number']=entities[1].strip()\n",
        "\n",
        "                elif \"work_experience\" in entities[0]:\n",
        "                    print(f\"Work_experience: {entities[1:]}\")\n",
        "                    dict_entities['Work_experience']=entities[1].strip()\n",
        "\n",
        "                elif \"projects\" in entities[0]:\n",
        "                    print(f\"projects: {entities[1:]}\")\n",
        "                    dict_entities['projects']=entities[1].strip()\n",
        "\n",
        "                elif \"email\" in entities[0]:\n",
        "                    print(f\"email: {entities[1]}\")\n",
        "                    dict_entities['email']=entities[1].strip()\n",
        "\n",
        "                elif \"gender\" in entities[0]:\n",
        "                    print(f\"gender: {entities[1]}\")\n",
        "                    dict_entities['gender']=entities[1].strip()\n",
        "\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "        #st.write(dict_entities)\n",
        "        #st.markdown(dict_entities)\n",
        "\n",
        "        #--------------------output to front-end---------------------------\n",
        "\n",
        "            st.markdown(\"### Details\")\n",
        "            col1,col2=st.columns(2)\n",
        "            with col1:\n",
        "                # st.markdown(\"**Number of pages**\")\n",
        "                st.text_input(label=\"Name\",value=dict_entities['name'])\n",
        "            with col2:\n",
        "                st.text_input(label=\"Phone Number\",value=dict_entities['mobile_number'].replace(\" \",\"\"))\n",
        "\n",
        "\n",
        "            st.text_area(label=\"Work experience\",value=dict_entities['Work_experience'])\n",
        "\n",
        "            st.text_area(label=\"Projects\",value=dict_entities['projects'])\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                # st.markdown(\"**Number of pages**\")\n",
        "                st.text_input(label=\"Email\", value=dict_entities['email'])\n",
        "            with col2:\n",
        "                st.text_input(label=\"Gender\", value=dict_entities['gender'])\n",
        "\n",
        "            # --------------------summary of uploaded file---------------------------\n",
        "            st.write(\"\")\n",
        "            st.markdown(\"### Summary\")\n",
        "\n",
        "            st.session_state.summary = file_summary(st.session_state.file_doc)\n",
        "            st.write(st.session_state.summary)\n",
        "        except Exception as e:\n",
        "            print(\"ERROR----------------- Home page - cleaning result/output to front-end/summary of uploaded file\")\n",
        "            print(e)\n",
        "            st.markdown(\"### Oops! Something went wrong, please try again\")\n",
        "            st.stop()\n",
        "\n",
        "        # --------------------Project QnA generation---------------------------\n",
        "\n",
        "        prompted_project_qna = prompt_project_qna.format(text=st.session_state.file_doc)\n",
        "\n",
        "        # llm = model(openai_key)\n",
        "\n",
        "        # if st.session_state.questions != None:\n",
        "        try:\n",
        "            st.session_state.questions = llm(prompted_project_qna)\n",
        "        except RateLimitError as e:\n",
        "            print(\"ERROR-----------------In Project Q&A Model\")\n",
        "            st.markdown(\"### Please enter a valid openai API key\")\n",
        "        # print(st.session_state.questions)\n",
        "\n",
        "\n",
        "\n",
        "        # # --------------------Interact with uploaded file---------------------------\n",
        "        #\n",
        "        # st.write(\"\")\n",
        "        # st.markdown(\"Ask a question\")\n",
        "        # # user_question=\"what is the name of the person\"\n",
        "        # user_question = st.text_input(label=\" \")\n",
        "        # if len(user_question) > 2:\n",
        "        #     user_question = str(user_question)\n",
        "        #\n",
        "        #     embeddings = OpenAIEmbeddings(openai_api_key=openai_key)\n",
        "        #     docsearch = Chroma.from_documents(file_doc, embeddings)\n",
        "        #\n",
        "        #     qa = VectorDBQA.from_chain_type(llm=llm, chain_type=\"stuff\", vectorstore=docsearch,\n",
        "        #                                     return_source_documents=False)\n",
        "        #     answer = qa({\"query\": user_question})\n",
        "        #\n",
        "        #     st.write(answer['result'])\n",
        "\n",
        "#-----------------------------------------Chat with Resume/CV-----------------------------\n",
        "\n",
        "elif choices==\"Chat with Resume/CV\":\n",
        "    st.markdown(\"### Profile Overview\")\n",
        "    st.write(st.session_state.summary)\n",
        "\n",
        "    # --------------------Interact with uploaded file---------------------------\n",
        "    try:\n",
        "        st.write(\"\")\n",
        "        st.markdown(\"Ask a question\")\n",
        "        # user_question=\"what is the name of the person\"\n",
        "        user_question = st.text_input(label=\" \")\n",
        "        if len(user_question) > 2:\n",
        "            user_question = str(user_question)\n",
        "\n",
        "            embeddings = OpenAIEmbeddings(openai_api_key=openai_key)\n",
        "            docsearch = Chroma.from_documents(st.session_state.file_doc, embeddings)\n",
        "            llm=model(openai_key)\n",
        "\n",
        "\n",
        "            qa = VectorDBQA.from_chain_type(llm=llm, chain_type=\"stuff\", vectorstore=docsearch,\n",
        "                                                return_source_documents=False)\n",
        "            answer = qa({\"query\": user_question})\n",
        "            st.write(answer['result'])\n",
        "\n",
        "    except RateLimitError as e:\n",
        "        print(\"ERROR----------------- In Interact with uploaded file\")\n",
        "        print(e)\n",
        "        st.markdown(\"### Oops! Something went wrong, please try again\")\n",
        "\n",
        "#-----------------------------------------Project Q&A-----------------------------\n",
        "elif choices==\"Project Q&A\":\n",
        "    st.markdown(\"### Project Q&A\")\n",
        "    # prompted_project_qna = prompt_project_qna.format(text=st.session_state.file_doc)\n",
        "    #\n",
        "    # llm = model(openai_key)\n",
        "    #\n",
        "    # # if st.session_state.questions != None:\n",
        "    # try:\n",
        "    #     st.session_state.questions = llm(prompted_project_qna)\n",
        "    # except RateLimitError as e:\n",
        "    #     print(\"ERROR-----------------In Project Q&A Model\")\n",
        "    #     st.markdown(\"### Please enter a valid openai API key\")\n",
        "    # print(st.session_state.questions)\n",
        "    try:\n",
        "        splitted_questions = st.session_state.questions.split('\\n')\n",
        "        # print(splitted_questions)\n",
        "\n",
        "        dict_project_qna = {}\n",
        "        widget_id = (i for i in range((len(splitted_questions) - 1)))\n",
        "        for i in range(len(splitted_questions)):\n",
        "            if i == 0:\n",
        "                continue\n",
        "\n",
        "            # print(splitted_questions[i])\n",
        "            # user_answer = input(\"Your answer: \")\n",
        "            # st.write()\n",
        "            st.session_state.user_answer = st.text_input(label=splitted_questions[i], key=f\"key_{i}\")\n",
        "\n",
        "            dict_project_qna[splitted_questions[i]] = st.session_state.user_answer\n",
        "    except Exception as e:\n",
        "        print(\"ERROR----------------- In Project Q&A-------\")\n",
        "        print(e)\n",
        "        st.markdown(\"### Please check upload your file in Home\")\n",
        "\n",
        "\n",
        "    if st.button(\"Finish\"):\n",
        "        print(dict_project_qna)\n",
        "        print(st.session_state.questions)\n",
        "        # st.success(\"Thank you for the response :-)\")\n",
        "        # st.session_state.questions = None\n",
        "        # questions = st.session_state.questions\n",
        "\n",
        "        with open(\"project_QnA_response\",\"w\") as f:\n",
        "            for i in dict_project_qna:\n",
        "                f.write(f\"\\nquestion {i} | answer {dict_project_qna[i]} \\n\")\n",
        "\n",
        "        thanks_message = st.empty()\n",
        "        thanks_message.success(\"Thanks for your response!\")\n",
        "        # st.stop()\n",
        "\n",
        "        # Clear session state\n",
        "        # st.session_state.clear()\n",
        "\n",
        "#     ------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tb9FWsWQ8Vvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rough use cell"
      ],
      "metadata": {
        "id": "sTMREYA96knR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vv4eHv4bz9_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5RAKfz8h1rAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a24da5-5deb-4d2c-d243-fa3ed49f620f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.91.1.38"
          ]
        }
      ],
      "source": [
        "!curl ifconfig.me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkYM1jhK2t4t"
      },
      "source": [
        "# Tunnel Opener"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pVI6qZK2VaU"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyODtupCnFXuMwfI+uJ31GHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}