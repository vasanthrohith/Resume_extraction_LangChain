{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasanthrohith/Resume_extraction_LangChain/blob/main/App_ResumeExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugqfZSUw0bXj"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install unstructured\n",
        "!pip install tiktoken\n",
        "!pip install pypdf2\n",
        "!pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvUs435SjHAf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEzQlPQ11BZM",
        "outputId": "cd642685-1dc0-485f-fc14-84dc43e01ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0YUpMXX1G-g"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki1iDOAk2nUM"
      },
      "source": [
        "# Main Shell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQLWs1Fm1qnU",
        "outputId": "643d8a48-d89b-4749-e333-203991843b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import PyPDF2\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "openai_key=\"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\"\n",
        "\n",
        "template=\"\"\"\n",
        "You will be provided with text which are extracted from resume\n",
        "your goal is to extract the below detail from the provided text\n",
        "- Name of the person\n",
        "- mobile number\n",
        "- Work experience\n",
        "- projects\n",
        "- email address\n",
        "\n",
        "if there is any field mentioned above not in text you should simply mention NA\n",
        "\n",
        "You should not consider skills and area of interest as work experience\n",
        "if there is no work experience simply mention NA\n",
        "\n",
        "in addition you should predict the gender of that person and mention it as\n",
        "- gender\n",
        "\n",
        "Below is the text:\n",
        "text: {text}\n",
        "\n",
        "Your resopnse:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt=PromptTemplate(input_variables=[\"text\"],\n",
        "                      template=template)\n",
        "\n",
        "\n",
        "st.header(\"Get your resume highlights\")\n",
        "# st.write(\"upload your file\")\n",
        "\n",
        "\n",
        "def model(openai_key):\n",
        "    llm=OpenAI(temperature=0,openai_api_key=openai_key)\n",
        "    return llm\n",
        "\n",
        "\n",
        "def path():\n",
        "    file_path_=\"\"\n",
        "    #st.write(file_path_)\n",
        "    time.sleep(3)\n",
        "    try:\n",
        "        while True:\n",
        "            current_path=os.getcwd()\n",
        "            file_name=\"saved_file.txt\"\n",
        "            file_path=os.path.join(current_path,file_name)\n",
        "            file_path_ = file_path\n",
        "            break\n",
        "    except Exception as e:\n",
        "        path()\n",
        "    return file_path_\n",
        "\n",
        "\n",
        "\n",
        "uploaded_file = st.file_uploader(\"upload your file\", type=['pdf'])\n",
        "\n",
        "if uploaded_file:\n",
        "    #st.write(uploaded_file.type)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #--------------------File handling---------------------------\n",
        "    text=\"\"\n",
        "    pdf1 = PyPDF2.PdfReader(uploaded_file)\n",
        "    pages = len(pdf1.pages)\n",
        "    print(\"Number of pages - \",pages)\n",
        "    for i in range(pages):\n",
        "        page=pdf1.pages[i]\n",
        "        text+=page.extract_text()\n",
        "\n",
        "\n",
        "    #current_path=os.getcwd()\n",
        "    #file_name=\"saved_file.txt\"\n",
        "    #file_path=os.path.join(current_path,file_name)\n",
        "\n",
        "    file_path=path()\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        os.remove(file_path)\n",
        "        with open(\"saved_file.txt\", \"a\") as f:\n",
        "            f.write(text)\n",
        "            #st.success(\"File saved successfully.\")\n",
        "\n",
        "    else:\n",
        "        with open(\"saved_file.txt\", \"a\") as f:\n",
        "            f.write(text)\n",
        "            #st.success(\"File saved successfully.\")\n",
        "\n",
        "    #file_path=path()\n",
        "    #st.write(file_path)\n",
        "\n",
        "    loader=TextLoader(file_path)\n",
        "    file_doc=loader.load()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #--------------------Fetching Details---------------------------\n",
        "\n",
        "\n",
        "\n",
        "    llm=model(openai_key)\n",
        "\n",
        "    try:\n",
        "        prompted=prompt.format(text=file_doc)\n",
        "        result=llm(prompted)\n",
        "\n",
        "    except Exception as e:\n",
        "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=2000,shunk_overlap=0)\n",
        "        file_doc=text_splitter.split_documents(file_load)\n",
        "\n",
        "        summarizer=load_summarize_chain(llm,chain_type=\"map_reduce\",verbose=True)\n",
        "        file_doc_summary=file_summarizer.run(docs)\n",
        "\n",
        "        prompted=prompt.format(text=file_doc_summary)\n",
        "        result=llm(prompted)\n",
        "\n",
        "\n",
        "    #st.write(result)\n",
        "\n",
        "    #--------------------Cleaning result---------------------------\n",
        "\n",
        "    splitted=result.split('\\n')\n",
        "    # print(splitted)\n",
        "\n",
        "    #split(\\n)=> to separate every entities in newline so that easily iterated through\n",
        "\n",
        "    dict_entities={}\n",
        "    # using dict to collect all the entities for easy accessibility\n",
        "\n",
        "    for i in splitted:\n",
        "        i=i.lower()\n",
        "        # lower() => to make normalize the text\n",
        "\n",
        "        entities=i.split(':')\n",
        "        # split(':') => to separate label(entities[0]) and values(entities[1])\n",
        "\n",
        "        # print(entities)\n",
        "\n",
        "        #below ladder if will help us to extract every entities and add to dict_entities based on it's label\n",
        "\n",
        "        if \"name\" and \"person\" in entities[0]:\n",
        "            print(f\"name: {entities[1]}\")\n",
        "            dict_entities['name']=entities[1].strip()\n",
        "\n",
        "        elif \"mobile\" and \"number\" in entities[0]:\n",
        "            print(f\"mobile_number: {entities[1]}\")\n",
        "            dict_entities['mobile_number']=entities[1].strip()\n",
        "\n",
        "        elif \"work experience\" in entities[0]:\n",
        "            print(f\"Work_experience: {entities[1]}\")\n",
        "            dict_entities['Work_experience']=entities[1].strip()\n",
        "\n",
        "        elif \"projects\" in entities[0]:\n",
        "            print(f\"projects: {entities[1]}\")\n",
        "            dict_entities['projects']=entities[1].strip()\n",
        "\n",
        "        elif \"email\" in entities[0]:\n",
        "            print(f\"email: {entities[1]}\")\n",
        "            dict_entities['email']=entities[1].strip()\n",
        "\n",
        "        elif \"gender\" in entities[0]:\n",
        "            print(f\"gender: {entities[1]}\")\n",
        "            dict_entities['gender']=entities[1].strip()\n",
        "\n",
        "\n",
        "    #st.write(dict_entities)\n",
        "    st.markdown(\"### Details\")\n",
        "\n",
        "    col1,col2=st.columns(2)\n",
        "    with col1:\n",
        "        # st.markdown(\"**Number of pages**\")\n",
        "        st.text_input(label=\"Name\",value=dict_entities['name'])\n",
        "    with col2:\n",
        "        st.text_input(label=\"Phone Number\",value=dict_entities['mobile_number'].replace(\" \",\"\"))\n",
        "\n",
        "\n",
        "    st.text_area(label=\"Work experience\",value=dict_entities['Work_experience'])\n",
        "\n",
        "    st.text_area(label=\"Projects\",value=dict_entities['projects'])\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        # st.markdown(\"**Number of pages**\")\n",
        "        st.text_input(label=\"Email\", value=dict_entities['email'])\n",
        "    with col2:\n",
        "        st.text_input(label=\"Gender\", value=dict_entities['gender'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    if os.path.exists(file_path):\n",
        "        os.remove(file_path)\n",
        "        with open(\"saved_file.txt\", \"a\") as f:\n",
        "            f.write(text)\n",
        "            #st.success(\"File saved successfully.\")\n",
        "\n",
        "    else:\n",
        "        with open(\"saved_file.txt\", \"a\") as f:\n",
        "            f.write(text)\n",
        "            #st.success(\"File saved successfully.\")"
      ],
      "metadata": {
        "id": "Vv4eHv4bz9_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RAKfz8h1rAL",
        "outputId": "e1059907-0048-4e28-e8c4-39bdc9188ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.231.120.55"
          ]
        }
      ],
      "source": [
        "!curl ifconfig.me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkYM1jhK2t4t"
      },
      "source": [
        "# Tunnel Opener"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pVI6qZK2VaU",
        "outputId": "ac4837ef-829e-403d-f1a4-3a7fc974ebd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l[..................] / rollbackFailedOptional: verb npm-session 99bc3b01c19d6d3\u001b[0m\u001b[K\r[..................] / rollbackFailedOptional: verb npm-session 99bc3b01c19d6d3\u001b[0m\u001b[K\r[..................] / rollbackFailedOptional: verb npm-session 99bc3b01c19d6d3\u001b[0m\u001b[K\r[..................] / rollbackFailedOptional: verb npm-session 99bc3b01c19d6d3\u001b[0m\u001b[K\r[..................] / rollbackFailedOptional: verb npm-session 99bc3b01c19d6d3\u001b[0m\u001b[K\r[..................] / rollbackFailedOptional: verb npm-session 99bc3b01c19d6d3\u001b[0m\u001b[K\r\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.231.120.55:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.413s\n",
            "your url is: https://flat-points-happen.loca.lt\n",
            "Number of pages -  1\n",
            "name:  vasanth shankar n\n",
            "mobile_number:  9952463 879\n",
            "Work_experience:  application engineer at newgen softwares and technologies india, artificial intelligence trainee at imagecon india private limited\n",
            "projects:  petrofac - product deployment and server management, prdp - product development and support, text to speech / speech to text convertor, one-supermart-agrocery store data management app, beetle ai - personal assistant works on voice command, calorie calculator - personal trainer app, deployment - all in one prediction app, flower family prediction (knn and naive bayes), sentiment analysis - using nlp and text mining, webscraping - fetching the datas of specified products from amazon and flipkart\n",
            "email:  vasanthrohith777@gmail.com\n",
            "gender:  male\n",
            "Number of pages -  2\n",
            "name:  vasanth shankar n\n",
            "mobile_number:  9952463879\n",
            "Work_experience:  na\n",
            "projects:  treatment of tannery effluent using advanced oxidation method, waste to energy conversion\n",
            "email:  reachvasanthshankarn@gmail.com\n",
            "gender:  male\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMEwOY18nzoHrlV+UImD/TK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}